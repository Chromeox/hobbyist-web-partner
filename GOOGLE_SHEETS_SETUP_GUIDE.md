# ğŸ“Š Google Sheets Data Hub Setup Guide

## Quick Start (10 minutes)

### Step 1: Create Your Google Sheet
1. Go to [sheets.google.com](https://sheets.google.com)
2. Create new spreadsheet: **"Hobby Directory Data Hub"**
3. Copy the spreadsheet ID from the URL:
   - URL: `https://docs.google.com/spreadsheets/d/SPREADSHEET_ID_HERE/edit`
   - Copy the ID between `/d/` and `/edit`

### Step 2: Install the Apps Script
1. In your sheet, go to **Extensions â†’ Apps Script**
2. Delete the default code
3. Copy ALL content from `sheets-setup.js`
4. Paste into Apps Script editor
5. Click **Save** (ğŸ’¾ icon)
6. Click **Run** â†’ Select `setupHobbyDirectory`
7. Grant permissions when prompted
8. You'll see "âœ… Hobby Directory setup complete!"

### Step 3: Verify Setup
Your sheet should now have these tabs:
- âœ… Events Staging (main data)
- âœ… Instagram Queue
- âœ… Website Queue
- âœ… Review Queue
- âœ… Published
- âœ… Settings
- âœ… Error Log

Plus a custom menu: **ğŸ¯ Hobby Directory**

---

## ğŸ”Œ Connect Playwright Scraper

### Option A: Simple Web Apps Script (No API Key Needed!)
1. In Apps Script, click **Deploy** â†’ **New Deployment**
2. Type: **Web app**
3. Execute as: **Me**
4. Who has access: **Anyone** (or "Anyone with Google Account")
5. Click **Deploy**
6. Copy the Web App URL (save this!)

Now Playwright can POST data directly to this URL:

```javascript
// In your Playwright script
const response = await fetch('YOUR_WEB_APP_URL', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    name: 'Pottery Class',
    date: '2024-01-15',
    venue: 'Claymates Studio',
    // ... other fields
  })
});
```

### Option B: Google Sheets API (More Complex)
1. Go to [Google Cloud Console](https://console.cloud.google.com)
2. Create new project or select existing
3. Enable Google Sheets API
4. Create Service Account credentials
5. Download JSON key file
6. Share your sheet with the service account email

---

## ğŸ“ Using the Sheet

### Daily Workflow

#### Morning (Automated)
- 7:30 AM: Review email sent automatically
- Contains all pending events
- Click links to approve/reject

#### Manual Actions via Menu
- **ğŸ“¥ Import Instagram Data**: Add Instagram handles to scrape
- **ğŸŒ Scrape Websites**: Run IMPORTXML formulas
- **ğŸ¤– Rewrite Descriptions**: Process with AI (mock function included)
- **âœ… Approve Selected**: Mark events as approved
- **ğŸ“¤ Export to CSV**: Generate CSV for Airtable import

### IMPORTXML Magic (Built-in Web Scraping!)
The Website Queue sheet includes these formulas:
```
=IMPORTXML(B2,"//title")                                    # Page title
=IMPORTXML(B2,"//meta[@name='description']/@content")       # Meta description  
=IMPORTXML(B2,"//meta[@property='og:image']/@content")      # Social image
```

These run automatically when you add URLs!

---

## ğŸš€ Test the System

### 1. Test Manual Entry
1. Go to **Events Staging** sheet
2. Add a test row:
   - Name: "Test Pottery Class"
   - Date: "2024-01-20"
   - Venue: "Claymates Studio"
   - Status: "Scraped"
3. Run **ğŸ¯ Hobby Directory â†’ Rewrite Descriptions**
4. Check that Description_Rewritten was filled

### 2. Test Web Scraping
1. Go to **Website Queue** sheet
2. Add a URL in column B: `https://www.claymatesceramicsstudio.com`
3. Wait 5 seconds for IMPORTXML
4. Run **ğŸ¯ Hobby Directory â†’ Scrape Websites**
5. Check columns D-F for scraped data

### 3. Test Review Email
1. Add a few test events with status "Pending Review"
2. Run **ğŸ¯ Hobby Directory â†’ Send Review Email**
3. Check your email for the review digest

---

## ğŸ”§ Configuration (Settings Sheet)

Customize these values:
- **Review_Time**: When to send daily email (24hr format)
- **Review_Email**: Where to send reviews
- **Target_Accounts**: Instagram accounts to scrape
- **Webhook_URL**: Your AI rewriting endpoint (optional)

---

## ğŸ“Š Data Flow

```
Instagram/Websites
       â†“
Playwright Scraper
       â†“
Google Sheets (via Web App or API)
       â†“
AI Rewriting (Apps Script)
       â†“
Daily Review Email (7:30 AM)
       â†“
Manual Approval
       â†“
Export CSV
       â†“
Import to Airtable
       â†“
WhaleSync â†’ Webflow
```

---

## ğŸ¯ Next Steps

1. **Set up Playwright scraper**:
   ```bash
   cd ~/HobbyistSwiftUI
   npm install playwright googleapis dotenv
   node playwright-to-sheets.js
   ```

2. **Schedule daily runs**:
   - Use cron (Mac/Linux) or Task Scheduler (Windows)
   - Or deploy to cloud (Heroku, Google Cloud Functions)

3. **Connect to Airtable**:
   - Export approved events as CSV
   - Import to Airtable (can be automated with Playwright!)
   - WhaleSync picks up from there

---

## ğŸ’¡ Pro Tips

1. **Bulk Operations**: Select multiple rows and use menu actions
2. **Filters**: Create filter views for different statuses
3. **Conditional Formatting**: Already set up for visual status tracking
4. **Version History**: Sheets auto-saves all changes with history
5. **Collaboration**: Share with team for review/approval
6. **Mobile**: Use Google Sheets app for on-the-go reviews

---

## ğŸ†˜ Troubleshooting

| Issue | Solution |
|-------|----------|
| "You do not have permission" | Re-run setup and grant all permissions |
| IMPORTXML returns error | Check if website blocks scrapers, try different formula |
| Email not sending | Check Settings sheet for correct email |
| Triggers not working | Go to Apps Script â†’ Triggers â†’ Add manually |
| Web App returns 404 | Redeploy and get new URL |

---

*This setup gives you 90% of Airtable's functionality for FREE, plus built-in web scraping!*